{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0P1SF7j3Tfa"
      },
      "outputs": [],
      "source": [
        "!pip install llama-cpp-python\n",
        "!pip install -U bitsandbytes\n",
        "!pip install human-eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orHsLX26UZ3A"
      },
      "outputs": [],
      "source": [
        "from llama_cpp import Llama\n",
        "from human_eval.data import write_jsonl, read_problems\n",
        "from human_eval.evaluation import evaluate_functional_correctness\n",
        "\n",
        "llm = Llama.from_pretrained(\n",
        "    repo_id=\"samzito12/lora_model5\", # Specify the Hugging Face repo ID\n",
        "    filename=\"model_gguf.gguf\",              # Specify the filename pattern for your model file within the repo\n",
        "    n_gpu_layers=100,                # 4GB GPU: commence à 40 (monte à 60 si stable)\n",
        "    n_ctx=1024,                     # réduit la mémoire vs 4096\n",
        ")\n",
        "\n",
        "def generate_one_completion(prompt: str, max_new_tokens: int = 512) -> str:\n",
        "    out = llm(\n",
        "        prompt,\n",
        "        max_tokens=max_new_tokens,\n",
        "        temperature=0,\n",
        "        stop=[\"\\n\\n\"],\n",
        "    )\n",
        "    return out[\"choices\"][0][\"text\"]\n",
        "\n",
        "\n",
        "problems = read_problems()\n",
        "num_samples_per_task = 1  # pass@1\n",
        "samples = [\n",
        "    dict(task_id=task_id, completion=generate_one_completion(problems[task_id][\"prompt\"]))\n",
        "    for task_id in problems\n",
        "    for _ in range(num_samples_per_task)\n",
        "]\n",
        "write_jsonl(\"samples.jsonl2\", samples)\n",
        "print(\"Wrote samples.jsonl2\")\n",
        "\n",
        "results = evaluate_functional_correctness(\"samples.jsonl2\")\n",
        "print(results)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}